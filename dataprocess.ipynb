{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import time\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from shutil import copyfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cap(filepath):\n",
    "    cap_dict = {}\n",
    "    with open(filepath) as cap:\n",
    "        for line in cap:\n",
    "            line_split= line.split('\\t', 1) #\\t is tab; maxsplit is 1\n",
    "            caption = line_split[1][:-1]\n",
    "            imgid=line_split[0].split(sep='#')[0]\n",
    "            if imgid not in cap_dict:\n",
    "                cap_dict[imgid]=[caption]\n",
    "            else:\n",
    "                cap_dict[imgid].append(caption)\n",
    "    return cap_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_cap = 'f8k_cap'\n",
    "filename_token = 'Flickr8k.token.txt'\n",
    "filepath_token = os.path.join(dir_cap, filename_token)\n",
    "cap_dict = read_cap(filepath_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A child in a pink dress is climbing up a set of stairs in an entry way .',\n",
       " 'A girl going into a wooden building .',\n",
       " 'A little girl climbing into a wooden playhouse .',\n",
       " 'A little girl climbing the stairs to her playhouse .',\n",
       " 'A little girl in a pink dress going into a wooden cabin .']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cap_dict['1000268201_693b08cb0e.jpg']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_id(filepath):\n",
    "    ids = []\n",
    "    with open(filepath) as file:\n",
    "        for line in file:\n",
    "            ids.append(line[:-1])\n",
    "    return ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copyfiles(dir_output, dir_input, ids):\n",
    "    if not os.path.exists(dir_output):\n",
    "        os.makedirs(dir_output)\n",
    "    for i in ids:\n",
    "        path_in=os.path.join(dir_input, i)\n",
    "        path_out=os.path.join(dir_output, i)\n",
    "        copyfile(path_in, path_out)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_cap(dir_output, ids, cap_dict):\n",
    "    path_out=os.path.join(dir_output, 'captions.txt')\n",
    "    output=[]\n",
    "    for i in ids:\n",
    "        dic={i: cap_dict[i]}\n",
    "        output.append(json.dumps(i))\n",
    "    with open(path_out, mode='w') as file:\n",
    "            file.write('\\n'.join(output))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cap(cap_dir):\n",
    "    cap=os.path.join(cap_dir, 'captions.txt')\n",
    "    cap_dict={}\n",
    "    with open(cap) as cap:\n",
    "        for line in cap:\n",
    "            dic=json.loads(line)\n",
    "            for i, j in dic.items():\n",
    "                cap_dict[i]=j\n",
    "    return cap_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def re_allocate(dir_img, token, cap_path):\n",
    "    dir_out={'train': 'train', 'val': 'val', 'test': 'test'}\n",
    "    \n",
    "    cap_dict = read_cap(token) #get caption dictionary \n",
    "    \n",
    "    img=os.listdir(dir_img) #train, val, test mix; all img\n",
    "    \n",
    "    id_train=get_id(cap_path['train']) #ger ids\n",
    "    id_val =get_id(cap_path['val']) \n",
    "    id_test=get_id(cap_path['test'])\n",
    "    \n",
    "    copyfiles(dir_out['train'], dir_img, id_train) #sort files to new dir\n",
    "    copyfiles(dir_out['val'], dir_img, id_val)\n",
    "    copyfiles(dir_out['test'], dir_img ,id_test)\n",
    "    \n",
    "    write_cap(dir_out['train'], id_train, cap_dict)\n",
    "    write_cap(dir_out['val'], id_val, cap_dict)\n",
    "    write_cap(dir_out['test'], id_test, cap_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_img='f8k_img'\n",
    "dir_text='f8k_text'\n",
    "file_token='Flickr8k.token.txt'\n",
    "file_train='Flickr_8k.trainImages.txt'\n",
    "file_val='Flickr_8k.devImages.txt'\n",
    "file_test='Flickr_8k.testImages.txt'\n",
    "filepath_token=os.path.join(dir_text, file_token)\n",
    "\n",
    "cap_path={'train': os.path.join(dir_text, file_train), 'val': os.path.join(dir_text, file_val), \n",
    "         'test': os.path.join(dir_text, file_test)}\n",
    "\n",
    "re_allocate(dir_img, filepath_token, cap_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "class Alexnet(nn.Module):\n",
    "    def __init__(self, embedding_dim=512):\n",
    "        super(Alexnet, self).__init__()\n",
    "        self.alexnet = models.alexnet(pretrained=True)\n",
    "        in_features = self.alexnet.classifier[6].in_features\n",
    "        self.linear = nn.Linear(in_features, embedding_dim)\n",
    "        self.alexnet.classifier[6] = self.linear\n",
    "        # self.batch_norm = nn.BatchNorm1d(embedding_dim, momentum=0.01)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.linear.weight.data.normal_(0.0, 0.02)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "    \n",
    "    def forward(self, images):\n",
    "        embed = self.alexnet(images)\n",
    "        # embed = Variable(embed.data)\n",
    "        # embed = embed.view(embed.size(0), -1)\n",
    "        # embed = self.linear(embed)\n",
    "        # embed = self.batch_norm(embed)\n",
    "        return embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size):\n",
    "        super(RNN, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n",
    "        self.linear = nn.Linear(hidden_dim, vocab_size)\n",
    "        self.init_weights()\n",
    "    \n",
    "    def init_weights(self):\n",
    "        self.word_embeddings.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.weight.data.uniform_(-0.1, 0.1)\n",
    "        self.linear.bias.data.fill_(0)\n",
    "        \n",
    "    def forward(self, features, caption):\n",
    "        seq_length = len(caption) + 1\n",
    "        embeds = self.word_embeddings(caption)\n",
    "        embeds = torch.cat((features, embeds), 0)\n",
    "        lstm_out, _ = self.lstm(embeds.unsqueeze(1))\n",
    "        out = self.linear(lstm_out.view(seq_length, -1))\n",
    "        return out\n",
    "\n",
    "    '''def greedy(self, cnn_out, seq_len = 20):\n",
    "        ip = cnn_out\n",
    "        hidden = None\n",
    "        ids_list = []\n",
    "        for t in range(seq_len):\n",
    "            lstm_out, hidden = self.lstm(ip.unsqueeze(1), hidden)\n",
    "            # generating single word at a time\n",
    "            linear_out = self.linear(lstm_out.squeeze(1))\n",
    "            word_caption = linear_out.max(dim=1)[1]\n",
    "            ids_list.append(word_caption)\n",
    "            ip = self.word_embeddings(word_caption)\n",
    "        return ids_list'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
